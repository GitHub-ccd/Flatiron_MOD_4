{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 33 - PCA lessons and labs\n",
    "\n",
    "\n",
    "[pca-introduction](./dsc-pca-introduction-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "\n",
    "\n",
    "[unsupervised-learning](./dsc-unsupervised-learning-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "- Define unsupervised learning \n",
    "- Compare and contrast supervised and unsupervised learning \n",
    "- Identify real-world scenarios in which you would use unsupervised learning \n",
    "\n",
    "[curse-of-dimensionality](./dsc-curse-of-dimensionality-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "- Explain what is meant by the curse of dimensionality and its implications when training machine learning algorithms\n",
    "\n",
    "[curse-of-dimensionality-lab](./dsc-curse-of-dimensionality-lab-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "- Create and interpret a visual demonstrating how sparsity changes with n for n-dimensional spaces \n",
    "- Demonstrate how training time increases exponentially as the number of features increases\n",
    "\n",
    "[pca-in-scikitlearn](./dsc-pca-in-scikitlearn-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "- Explain at a high level how PCA works \n",
    "- Explain use cases for PCA \n",
    "- Implement PCA using the scikit-learn library \n",
    "- Determine the optimal number of n components when performing PCA by observing the explained variance \n",
    "\n",
    "[pca-in-scikitlearn-lab](./dsc-pca-in-scikitlearn-lab-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "Steps for Performing PCA\n",
    "\n",
    "The theory behind PCA rests upon many foundational concepts of linear algebra. After all, PCA is re-encoding a dataset into an alternative basis (the axes). Here are the exact steps:\n",
    "\n",
    "1. Recenter each feature of the dataset by subtracting that feature's mean from the feature vector\n",
    "2. Calculate the covariance matrix for your centered dataset\n",
    "3. Calculate the eigenvectors of the covariance matrix\n",
    "    1. You'll further investigate the concept of eigenvectors in the upcoming lesson\n",
    "4. Project the dataset into the new feature space: Multiply the eigenvectors by the mean-centered features\n",
    "\n",
    "[pca-and-pipelines-v2-1](./dsc-pca-and-pipelines-v2-1-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "\n",
    "\n",
    "[pca-covariance-matrix-eigendecomp](./dsc-pca-covariance-matrix-eigendecomp-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "\n",
    "\n",
    "[performing-principal-component-analysis](./dsc-performing-principal-component-analysis-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "\n",
    "\n",
    "[pca-numpy-lab](./dsc-pca-numpy-lab-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "\n",
    "\n",
    "[pca-and-digital-image-processing](./dsc-pca-and-digital-image-processing-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "\n",
    "\n",
    "[pca-and-digital-image-processing-lab](./dsc-pca-and-digital-image-processing-lab-onl01-dtsc-ft-030220/index.ipynb)<br>\n",
    "\n",
    "\n",
    "[pca-summary](./dsc-pca-summary-onl01-dtsc-ft-030220/index.ipynb)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
